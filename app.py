import os
import uuid
import logging
import base64
import re
from flask import Flask, request, jsonify
from flask_cors import CORS
import google.generativeai as genai
from functools import lru_cache

# ConfiguraciÃ³n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configurar la API de Gemini
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

app = Flask(__name__)
CORS(app)

# Estado por sesiÃ³n
session_steps = {}
session_data = {}
session_admin = {}

SYSTEM_PROMPT = '''
Eres una inteligencia artificial mÃ©dica especializada en apoyar a mÃ©dicos en la evaluaciÃ³n y comparaciÃ³n de diagnÃ³sticos. Tu objetivo es proporcionar anÃ¡lisis clÃ­nicos basados en la informaciÃ³n suministrada por el profesional de la salud, para ayudar a confirmar, descartar o ampliar hipÃ³tesis diagnÃ³sticas. No estÃ¡s autorizada para sustituir el juicio del mÃ©dico, solo para complementarlo.
'''

questions = {
    1: "ğŸ‘¤ Edad del paciente:",
    2: "ğŸš» Sexo asignado al nacer y gÃ©nero actual:",
    3: "ğŸ“ Motivo principal de consulta:",
    4: "â³ Â¿Desde cuÃ¡ndo presenta estos sÃ­ntomas? Â¿Han cambiado con el tiempo?",
    5: "ğŸ“‹ Antecedentes mÃ©dicos personales (crÃ³nicos, quirÃºrgicos, etc.):",
    6: "ğŸ’Š Medicamentos actuales (nombre, dosis, frecuencia):",
    7: "âš ï¸ Alergias conocidas (medicamentos, alimentos, etc.):",
    8: "ğŸ‘ª Antecedentes familiares relevantes:",
    9: "ğŸ§ª Estudios diagnÃ³sticos realizados y resultados si se conocen:"
}

@lru_cache(maxsize=100)
def get_cached_response(parts):
    model = genai.GenerativeModel("models/gemini-2.0-flash")
    response = model.generate_content(parts)
    return getattr(response, 'text', '').strip()

@app.route('/api/chat', methods=['POST'])
def chat():
    data       = request.json or {}
    session_id = data.get('session_id')
    user_msg   = data.get('message', '').strip()
    image_data = data.get('image')

    # â”€â”€ PRIMERA RAMA: si viene una imagen la procesamos de inmediato â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if image_data:
        # Aseguramos que exista la sesiÃ³n
        if not session_id or session_id not in session_steps:
            session_id = str(uuid.uuid4())
        session_steps.setdefault(session_id, 1)
        session_data.setdefault(session_id, [])
        # Activamos modo admin para permitir libertad tras la descripciÃ³n
        session_admin[session_id] = True

        # Decodificar la imagen y generar prompt a Gemini
        try:
            image_bytes = base64.b64decode(image_data.split(',')[-1])
            parts = [
                {"role": "system", "parts": [SYSTEM_PROMPT]},
                {"role": "user", "parts": [
                    {"inline_data": {"mime_type": "image/png", "data": image_bytes}},
                    # InstrucciÃ³n para el modelo
                    "Por favor, describe detalladamente lo que ves en esta imagen y luego pregunta al solicitante quÃ© te gustarÃ­a que haga a continuaciÃ³n."
                ]}
            ]
            ai_response = get_cached_response(tuple(map(str, parts)))
            return jsonify({"session_id": session_id, "response": ai_response})
        except Exception as e:
            logger.error("Error procesando imagen", exc_info=True)
            return jsonify({
                "session_id": session_id,
                "response": "âš ï¸ Hubo un error al procesar la imagen. Intenta de nuevo."
            })

    # â”€â”€ SEGUNDA RAMA: creaciÃ³n inicial de sesiÃ³n (si no existÃ­a) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not session_id or session_id not in session_steps:
        session_id = str(uuid.uuid4())
        session_steps[session_id] = 1
        session_data[session_id]  = []
        session_admin[session_id] = False
        # Iniciamos el cuestionario
        return jsonify({"session_id": session_id, "response": questions[1]})

    # â”€â”€ A partir de aquÃ­ ya tenemos session_id existente â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    step    = session_steps[session_id]
    is_admin= session_admin.get(session_id, False)

    # Activar modo admin si el usuario envÃ­a â€œadminâ€
    if user_msg.lower() == "admin":
        session_admin[session_id] = True
        return jsonify({
            "session_id": session_id,
            "response": "ğŸ”“ Modo Admin activado. Ahora puedes escribir libremente o subir imÃ¡genes."
        })

    # Validaciones y avances en el cuestionario
    def is_valid_response(text):
        if is_admin: return True
        if not text.strip(): return False
        if step == 1:
            return bool(re.search(r'\d{1,3}', text))
        if step == 2:
            return any(g in text.lower() for g in ["masculino","femenino","m","f","hombre","mujer"])
        return True

    if user_msg:
        if is_valid_response(user_msg):
            session_data[session_id].append(user_msg)
            if step < len(questions):
                session_steps[session_id] += 1
                return jsonify({
                  "session_id": session_id,
                  "response": questions[step+1]
                })
        else:
            return jsonify({
              "session_id": session_id,
              "response": "âš ï¸ Por favor, proporciona una respuesta vÃ¡lida."
            })

    # â”€â”€ Cuando terminamos el cuestionario o estamos en modo admin, generamos el informe â”€
    if step > len(questions) or is_admin:
        # Preparamos el informe clÃ­nico...
        info = "\n".join(f"{i+1}. {q}\nâ†’ {a}"
                         for i, (q,a) in enumerate(zip(questions.values(), session_data[session_id])))
        analysis_prompt = (
            f"ğŸ“ **Informe ClÃ­nico Detallado**\n\nğŸ“Œ Datos Recopilados:\n{info}\n\n"
            "ğŸ” **AnÃ¡lisis ClÃ­nico**\n"
            "Interpreta esta informaciÃ³n desde el punto de vista mÃ©dico y sugiere hipÃ³tesis diagnÃ³sticas "
            "posibles con base en evidencia cientÃ­fica, factores de riesgo y presentaciÃ³n del caso. "
            "Finaliza con recomendaciones para el mÃ©dico tratante."
        )
        parts = [
            {"role":"system","parts":[SYSTEM_PROMPT]},
            {"role":"user","parts":[analysis_prompt]}
        ]
        try:
            ai_response = get_cached_response(tuple(map(str, parts)))
            return jsonify({"session_id": session_id, "response": ai_response})
        except Exception as e:
            logger.error("Error en /api/chat:", exc_info=True)
            return jsonify({"error": str(e)}), 500

    # Por defecto, devolvemos la siguiente pregunta
    return jsonify({"session_id": session_id, "response": questions[step]})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port)
