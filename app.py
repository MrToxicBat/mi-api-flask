import os
import uuid
import logging
import base64
from flask import Flask, request, jsonify
from flask_cors import CORS
import google.generativeai as genai
from functools import lru_cache

# Configuraci√≥n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configurar la API de Gemini
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

app = Flask(__name__)
CORS(app)

session_steps = {}
session_data = {}

SYSTEM_PROMPT = '''
Eres una inteligencia artificial m√©dica especializada en apoyar a m√©dicos en la evaluaci√≥n y comparaci√≥n de diagn√≥sticos. Tu objetivo es proporcionar an√°lisis cl√≠nicos basados en la informaci√≥n suministrada por el profesional de la salud, para ayudar a confirmar, descartar o ampliar hip√≥tesis diagn√≥sticas. No est√°s autorizada para sustituir el juicio del m√©dico, solo para complementarlo.

Antes de generar cualquier diagn√≥stico diferencial, interpretaci√≥n o sugerencia, debes recopilar al menos la siguiente **informaci√≥n cl√≠nica b√°sica** del paciente:

1. Edad  
2. Sexo  
3. Motivo de consulta (s√≠ntoma principal, causa de la visita)  
4. Tiempo de evoluci√≥n de los s√≠ntomas  
5. Antecedentes personales patol√≥gicos (enfermedades previas, condiciones cr√≥nicas, cirug√≠as, etc.)  
6. Medicaci√≥n actual (principios activos o nombres comerciales, dosis si es posible)  
7. Alergias conocidas (medicamentosas, alimentarias, ambientales, etc.)  
8. Antecedentes familiares de enfermedades relevantes (gen√©ticas, cr√≥nicas o malignas)  
9. Estudios diagn√≥sticos realizados (an√°lisis cl√≠nicos, im√°genes, biopsias, etc., con resultados si se conocen)

‚ùóNo puedes emitir sugerencias si no tienes, como m√≠nimo: **edad**, **sexo** y **motivo de consulta**. Si faltan, ind√≠calo amablemente y solicita esa informaci√≥n antes de continuar.
'''

questions = {
    1: "üë§ Edad del paciente:",
    2: "üöª Sexo asignado al nacer y g√©nero actual:",
    3: "üìç Motivo principal de consulta:",
    4: "‚è≥ ¬øDesde cu√°ndo presenta estos s√≠ntomas? ¬øHan cambiado con el tiempo?",
    5: "üìã Antecedentes m√©dicos personales (cr√≥nicos, quir√∫rgicos, etc.):",
    6: "üíä Medicamentos actuales (nombre, dosis, frecuencia):",
    7: "‚ö†Ô∏è Alergias conocidas (medicamentos, alimentos, etc.):",
    8: "üë™ Antecedentes familiares relevantes:",
    9: "üß™ Estudios diagn√≥sticos realizados y resultados si se conocen:"
}

@lru_cache(maxsize=100)
def get_cached_response(parts):
    model = genai.GenerativeModel("models/gemini-2.0-flash")
    response = model.generate_content(parts)
    return getattr(response, 'text', '').strip()

@app.route('/api/chat', methods=['POST'])
def chat():
    data = request.json
    session_id = data.get('session_id')
    user_message = data.get('message', '').strip()
    image_data = data.get('image')

    if not session_id or session_id not in session_steps:
        session_id = str(uuid.uuid4())
        session_steps[session_id] = 1
        session_data[session_id] = []
        return jsonify({"session_id": session_id, "response": questions[1]})

    step = session_steps[session_id]

    if image_data and not user_message:
        return jsonify({"session_id": session_id, "response": questions[step]})

    if user_message:
        session_data[session_id].append(user_message)
        if step < len(questions):
            session_steps[session_id] += 1
            return jsonify({"session_id": session_id, "response": questions[step + 1]})

    if session_steps[session_id] > len(questions):
        respuestas = dict(zip(questions.values(), session_data[session_id]))
        edad = next((v for k, v in respuestas.items() if "Edad" in k), "")
        sexo = next((v for k, v in respuestas.items() if "Sexo" in k), "")
        motivo = next((v for k, v in respuestas.items() if "Motivo" in k), "")

        if not edad.strip() or not sexo.strip() or not motivo.strip():
            return jsonify({"session_id": session_id, "response": "‚ö†Ô∏è Necesito edad, sexo y motivo de consulta para poder continuar. Por favor, verifica que hayas respondido esas preguntas."})

        info = "\n".join(f"{i+1}. {q}\n‚Üí {a}" for i, (q, a) in enumerate(zip(questions.values(), session_data[session_id])))
        analysis_prompt = f"Gracias. A continuaci√≥n se presenta un informe cl√≠nico con base en la informaci√≥n suministrada.\n\n---\n\nüìù **Informe Cl√≠nico Detallado**\n\nüìå Datos Recopilados:\n{info}\n\nüîç **An√°lisis Cl√≠nico**\nPor favor, interpreta esta informaci√≥n desde el punto de vista m√©dico y sugiere hip√≥tesis diagn√≥sticas posibles con base en evidencia cient√≠fica, factores de riesgo, y la presentaci√≥n del caso. Finaliza con recomendaciones para el m√©dico tratante."

        parts = [
            {"role": "system", "parts": [SYSTEM_PROMPT]},
            {"role": "user", "parts": [analysis_prompt]}
        ]

        if image_data:
            try:
                image_bytes = base64.b64decode(image_data.split(',')[-1])
                parts[1]["parts"].append({"inline_data": {"mime_type": "image/png", "data": image_bytes}})
            except Exception as e:
                logger.warning("No se pudo procesar la imagen enviada.", exc_info=True)

        try:
            ai_response = get_cached_response(tuple(map(str, parts)))
            return jsonify({"session_id": session_id, "response": ai_response})
        except Exception as e:
            logger.error(f"Error en /api/chat: {e}", exc_info=True)
            return jsonify({"error": str(e)}), 500

    return jsonify({"session_id": session_id, "response": questions[step]})

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port)
