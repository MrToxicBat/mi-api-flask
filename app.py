import os
import uuid
import logging
import base64
from flask import Flask, request, jsonify
from flask_cors import CORS
import google.generativeai as genai
from functools import lru_cache

# ConfiguraciÃ³n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configurar la API de Gemini
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

app = Flask(__name__)
CORS(app)

session_steps = {}
session_data = {}

SYSTEM_PROMPT = '''
Eres una inteligencia artificial mÃ©dica especializada en apoyar a mÃ©dicos en la evaluaciÃ³n y comparaciÃ³n de diagnÃ³sticos. Tu objetivo es proporcionar anÃ¡lisis clÃ­nicos basados en la informaciÃ³n suministrada por el profesional de la salud, para ayudar a confirmar, descartar o ampliar hipÃ³tesis diagnÃ³sticas. No estÃ¡s autorizada para sustituir el juicio del mÃ©dico, solo para complementarlo.

Antes de generar cualquier diagnÃ³stico diferencial, interpretaciÃ³n o sugerencia, debes recopilar al menos la siguiente **informaciÃ³n clÃ­nica bÃ¡sica** del paciente:

1. Edad  
2. Sexo  
3. Motivo de consulta (sÃ­ntoma principal, causa de la visita)  
4. Tiempo de evoluciÃ³n de los sÃ­ntomas  
5. Antecedentes personales patolÃ³gicos (enfermedades previas, condiciones crÃ³nicas, cirugÃ­as, etc.)  
6. MedicaciÃ³n actual (principios activos o nombres comerciales, dosis si es posible)  
7. Alergias conocidas (medicamentosas, alimentarias, ambientales, etc.)  
8. Antecedentes familiares de enfermedades relevantes (genÃ©ticas, crÃ³nicas o malignas)  
9. Estudios diagnÃ³sticos realizados (anÃ¡lisis clÃ­nicos, imÃ¡genes, biopsias, etc., con resultados si se conocen)

â—No puedes emitir sugerencias si no tienes, como mÃ­nimo: **edad**, **sexo** y **motivo de consulta**. Si faltan, indÃ­calo amablemente y solicita esa informaciÃ³n antes de continuar.
'''

questions = {
    1: "ğŸ‘¤ Edad del paciente:",
    2: "ğŸš» Sexo asignado al nacer y gÃ©nero actual:",
    3: "ğŸ“ Motivo principal de consulta:",
    4: "â³ Â¿Desde cuÃ¡ndo presenta estos sÃ­ntomas? Â¿Han cambiado con el tiempo?",
    5: "ğŸ“‹ Antecedentes mÃ©dicos personales (crÃ³nicos, quirÃºrgicos, etc.):",
    6: "ğŸ’Š Medicamentos actuales (nombre, dosis, frecuencia):",
    7: "âš ï¸ Alergias conocidas (medicamentos, alimentos, etc.):",
    8: "ğŸ‘ª Antecedentes familiares relevantes:",
    9: "ğŸ§ª Estudios diagnÃ³sticos realizados y resultados si se conocen:"
}

@lru_cache(maxsize=100)
def get_cached_response(parts):
    model = genai.GenerativeModel("models/gemini-2.0-flash")
    response = model.generate_content(parts)
    return getattr(response, 'text', '').strip()

@app.route('/api/chat', methods=['POST'])
def chat():
    data = request.json
    session_id = data.get('session_id')
    user_message = data.get('message', '').strip()
    image_data = data.get('image')  # base64 opcional

    if not session_id or session_id not in session_steps:
        session_id = str(uuid.uuid4())
        session_steps[session_id] = 1
        session_data[session_id] = []
        return jsonify({
            "session_id": session_id,
            "response": questions[1]
        })

    session_data[session_id].append(user_message)
    step = session_steps[session_id]

    if step < len(questions):
        session_steps[session_id] += 1
        return jsonify({
            "session_id": session_id,
            "response": questions[step + 1]
        })
    else:
        respuestas = dict(zip(questions.values(), session_data[session_id]))
        edad = next((v for k, v in respuestas.items() if "Edad" in k), "")
        sexo = next((v for k, v in respuestas.items() if "Sexo" in k), "")
        motivo = next((v for k, v in respuestas.items() if "Motivo" in k), "")

        if not edad.strip() or not sexo.strip() or not motivo.strip():
            return jsonify({
                "session_id": session_id,
                "response": "âš ï¸ Necesito edad, sexo y motivo de consulta para poder continuar. Por favor, verifica que hayas respondido esas preguntas."
            })

        info = "\n".join(f"{i+1}. {q}\nâ†’ {a}" for i, (q, a) in enumerate(zip(questions.values(), session_data[session_id])))
        analysis_prompt = (
            f"Gracias. A continuaciÃ³n se presenta un informe clÃ­nico con base en la informaciÃ³n suministrada.\n\n"
            f"---\n\nğŸ“ **Informe ClÃ­nico Detallado**\n\nğŸ“Œ Datos Recopilados:\n{info}\n\n"
            "ğŸ” **AnÃ¡lisis ClÃ­nico**\nPor favor, interpreta esta informaciÃ³n desde el punto de vista mÃ©dico y sugiere hipÃ³tesis diagnÃ³sticas posibles con base en evidencia cientÃ­fica, factores de riesgo, y la presentaciÃ³n del caso. Finaliza con recomendaciones para el mÃ©dico tratante."
        )

        parts = [
            {"role": "system", "parts": [SYSTEM_PROMPT]},
            {"role": "user", "parts": [analysis_prompt]}
        ]

        if image_data:
            try:
                image_bytes = base64.b64decode(image_data.split(',')[-1])
                parts[1]["parts"].append({"inline_data": {"mime_type": "image/png", "data": image_bytes}})
            except Exception as e:
                logger.warning("No se pudo procesar la imagen enviada.", exc_info=True)

        try:
            ai_response = get_cached_response(tuple(map(str, parts)))
            return jsonify({
                "session_id": session_id,
                "response": ai_response
            })
        except Exception as e:
            logger.error(f"Error en /api/chat: {e}", exc_info=True)
            return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port)
